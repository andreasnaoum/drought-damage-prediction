{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreasnaoum/drought-damage-prediction/blob/main/Drought_Damage_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbjtYXN288O1"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3dOozj99BkB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76eOUoA58v2u"
      },
      "source": [
        "# Pre-requests for ML Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWCW_27J8lmt"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46xsOTSO86pE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTegbbPR9Hih"
      },
      "source": [
        "# Load Data & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOY1bUyJeZuO"
      },
      "outputs": [],
      "source": [
        "def resize_with_padding(image_path, target_size):\n",
        "    # Open the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Calculate the aspect ratio\n",
        "    original_width, original_height = img.size\n",
        "    aspect_ratio = original_width / original_height\n",
        "\n",
        "    # Determine the new size while maintaining aspect ratio\n",
        "    if aspect_ratio > 1:\n",
        "        new_width = target_size\n",
        "        new_height = int(target_size / aspect_ratio)\n",
        "    else:\n",
        "        new_height = target_size\n",
        "        new_width = int(target_size * aspect_ratio)\n",
        "\n",
        "    # Resize the image\n",
        "    resized_img = img.resize((new_width, new_height), Image.ANTIALIAS)\n",
        "\n",
        "    # Create a blank image with the target size\n",
        "    padded_img = Image.new(\"RGB\", (target_size, target_size), (255, 255, 255))\n",
        "\n",
        "    # Paste the resized image onto the blank image\n",
        "    x_offset = (target_size - new_width) // 2\n",
        "    y_offset = (target_size - new_height) // 2\n",
        "    padded_img.paste(resized_img, (x_offset, y_offset))\n",
        "\n",
        "    return padded_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KuYicV6ZqW4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Load the dataset\n",
        "data_path = \"/content/drive/MyDrive/\"\n",
        "train_data = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
        "\n",
        "modified_train_data = train_data.copy()\n",
        "\n",
        "\n",
        "# # Select a row from the dataset (you can change the index as needed)\n",
        "# row_index = 0\n",
        "# image_filename = modified_train_data.loc[row_index, 'filename']\n",
        "\n",
        "# # Construct the full path to the image\n",
        "# image_path = os.path.join(data_path, \"train\", image_filename)\n",
        "\n",
        "# # Load and display the image\n",
        "# img = Image.open(image_path)\n",
        "# plt.imshow(img)\n",
        "# plt.title(f\"Image: {image_filename}\")\n",
        "# plt.axis('off')  # Turn off axis labels\n",
        "# plt.show()\n",
        "\n",
        "# # Get and print the image size\n",
        "# image_size = img.size\n",
        "# print(f\"Image Size: {image_size}\")\n",
        "\n",
        "# # Specify the target size\n",
        "# target_size = 224\n",
        "\n",
        "# resized_image = resize_with_padding(image_path, target_size)\n",
        "\n",
        "# image_size = resized_image.size\n",
        "# print(f\"Image Size: {image_size}\")\n",
        "\n",
        "# # Display the original and resized images\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "# axes[0].imshow(Image.open(image_path))\n",
        "# axes[0].set_title(\"Original Image\")\n",
        "# axes[0].axis('off')\n",
        "# axes[1].imshow(resized_image)\n",
        "# axes[1].set_title(\"Resized Image with Padding\")\n",
        "# axes[1].axis('off')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xcc0A9sx0N2U"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def color_mask(img):\n",
        "  lower = np.array([205,133,63])\n",
        "  upper = np.array([255,255,224])\n",
        "\n",
        "  cv_img = np.array(img)\n",
        "  cv_img = cv_img[:, :, ::-1].copy()\n",
        "\n",
        "  mask = cv2.inRange(cv_img, lower, upper)\n",
        "\n",
        "  masked = cv2.bitwise_and(cv_img,cv_img, mask=mask)\n",
        "\n",
        "  return Image.fromarray(cv_img - masked)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkoxdvZvpA35"
      },
      "outputs": [],
      "source": [
        "# Specify the target size\n",
        "target_size = 224\n",
        "\n",
        "# Create a new folder for resized images\n",
        "output_folder = os.path.join(data_path, \"train_resized\")\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Resize and save each image to the new folder\n",
        "for row_index, row in train_data.iterrows():\n",
        "    image_filename = row['filename']\n",
        "    image_path = os.path.join(data_path, \"train\", image_filename)\n",
        "    output_path = os.path.join(output_folder, image_filename)\n",
        "\n",
        "    # Check if the resized image already exists\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"Resized image already exists: {output_path}\")\n",
        "    else:\n",
        "        # Check if the original file exists before attempting to open it\n",
        "        if os.path.exists(image_path):\n",
        "            resized_image = resize_with_padding(image_path, target_size)\n",
        "            masked_image = color_mask(resized_image)\n",
        "            # Save the resized image to the new folder\n",
        "            masked_image.save(output_path)\n",
        "        else:\n",
        "            print(f\"File not found: {image_path}\")\n",
        "\n",
        "print(\"Resizing and saving complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cy_bnjRR9Ln_"
      },
      "outputs": [],
      "source": [
        "# CNNs work bettet with values 0.0-1.0\n",
        "modified_train_data.extent = train_data.extent / 100\n",
        "\n",
        "# print(train_data.extent)\n",
        "# print(modified_train_data.extent)\n",
        "\n",
        "le_growth_stage = LabelEncoder()\n",
        "modified_train_data['growth_stage'] = le_growth_stage.fit_transform(modified_train_data['growth_stage'])\n",
        "num_growth_stages = len(le_growth_stage.classes_)\n",
        "\n",
        "le_damage = LabelEncoder()\n",
        "modified_train_data['damage'] = le_damage.fit_transform(modified_train_data['damage'])\n",
        "num_damage_types = len(le_damage.classes_)\n",
        "\n",
        "# One-hot encode the labels for classification tasks\n",
        "growth_stage_labels = to_categorical(modified_train_data['growth_stage'], num_classes=num_growth_stages)\n",
        "damage_type_labels = to_categorical(modified_train_data['damage'], num_classes=num_damage_types)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, val_df = train_test_split(modified_train_data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PW_2m459VwZ"
      },
      "outputs": [],
      "source": [
        "# Image data preprocessing\n",
        "image_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                  rescale=1./255,\n",
        "                  shear_range=0.2,\n",
        "                  zoom_range=0.2,\n",
        "                  horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "                      dataframe=train_df,\n",
        "                      directory=os.path.join(data_path, \"train_resized\"),\n",
        "                      x_col=\"filename\",\n",
        "                      y_col=\"extent\",\n",
        "                      target_size=image_size,\n",
        "                      batch_size=batch_size,\n",
        "                      class_mode=\"other\"\n",
        ")\n",
        "\n",
        "# testing\n",
        "# row_index = 0\n",
        "\n",
        "# # Create a generator for a single image\n",
        "# single_image_generator = train_datagen.flow_from_dataframe(\n",
        "#                             dataframe=train_data.iloc[[row_index]],\n",
        "#                             directory=os.path.join(data_path, \"train_resized\"),\n",
        "#                             x_col=\"filename\",\n",
        "#                             y_col=\"extent\",\n",
        "#                             target_size=image_size,\n",
        "#                             batch_size=1,\n",
        "#                             class_mode=\"other\"\n",
        "# )\n",
        "\n",
        "# # Get the preprocessed image and label\n",
        "# img, label = single_image_generator.next()\n",
        "\n",
        "# # Display the image\n",
        "# plt.imshow(img[0])\n",
        "# plt.title(f\"Image: {image_filename}, Label: {label[0]}\")\n",
        "# plt.axis('off')  # Turn off axis labels\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45ToCtKT9hoz"
      },
      "outputs": [],
      "source": [
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "                  dataframe=val_df,\n",
        "                  directory=os.path.join(data_path, \"train_resized\"),\n",
        "                  x_col=\"filename\",\n",
        "                  y_col=\"extent\",\n",
        "                  target_size=image_size,\n",
        "                  batch_size=batch_size,\n",
        "                  class_mode=\"other\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dQZAN0X9oO7"
      },
      "source": [
        "# Build CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXZU1B7C9qkK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(224, 224, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "print(\"Convonutional Base Model Summary:\")\n",
        "# Get summary of the model\n",
        "model.summary()\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Linear output node\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Alternative 11 Nodes - 0.1,0.2 - 1.0\n",
        "# model.add(Dense(11, activation='softmax'))\n",
        "\n",
        "# Different Idea\n",
        "\n",
        "# # Growth stage prediction branch\n",
        "# model.add(Dense(4, activation='softmax', name='growth_stage'))\n",
        "\n",
        "# # Damage type prediction branch\n",
        "# model.add(Dense(8, activation='softmax', name='damage_type'))\n",
        "\n",
        "# # Final output layer for extent prediction\n",
        "# model.add(Dense(11, activation='softmax', name='extent'))\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer=Adam(), loss={'growth_stage': 'categorical_crossentropy',\n",
        "#                                       'damage_type': 'categorical_crossentropy',\n",
        "#                                       'extent': 'mean_squared_error'},\n",
        "#               metrics={'growth_stage': 'accuracy',\n",
        "#                        'damage_type': 'accuracy'})\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "print(\"Final Model Summary:\")\n",
        "# Get summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_generator, epochs=5, validation_data=val_generator)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwILXJIi-NXF"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uVdtXw0rpJR"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "# model.save('/content/drive/MyDrive/model.h5')\n",
        "\n",
        "# If you want to load the model later\n",
        "from tensorflow.keras.models import load_model\n",
        "model = load_model('/content/drive/MyDrive/model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYkxfJYi-QEU"
      },
      "source": [
        "# Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt6EyM2K-T4c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Specify the path to the test directory\n",
        "test_directory = '/content/drive/MyDrive/test'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrwKilZVq450"
      },
      "outputs": [],
      "source": [
        "# Specify the target size\n",
        "target_size = 224\n",
        "\n",
        "# Create a new folder for resized test images\n",
        "output_folder = '/content/drive/MyDrive/test_resized'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Resize and save each image in the test folder to the new folder\n",
        "for image_filename in os.listdir(test_directory):\n",
        "    image_path = os.path.join(test_directory, image_filename)\n",
        "    output_path = os.path.join(output_folder, image_filename)\n",
        "\n",
        "    # Check if the resized image already exists\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"Resized image already exists: {output_path}\")\n",
        "    else:\n",
        "        # Check if the original file exists before attempting to open it\n",
        "        if os.path.exists(image_path):\n",
        "            resized_image = resize_with_padding(image_path, target_size)\n",
        "            masked_image = color_mask(resized_image)\n",
        "            # Save the resized image to the new folder\n",
        "            masked_image.save(output_path)\n",
        "        else:\n",
        "            print(f\"File not found: {image_path}\")\n",
        "\n",
        "print(\"Resizing and saving complete for test images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtBKXMX_rKwH"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(os.path.join(data_path, \"Test.csv\"))\n",
        "\n",
        "test_data['filename'] = test_data['filename'].str.replace('.JPG', '.jpg')\n",
        "\n",
        "num_test_images = len(test_data['filename'])\n",
        "print(f\"Found {num_test_images} images in the test directory.\")\n",
        "\n",
        "output_folder = '/content/drive/MyDrive/test_resized'\n",
        "\n",
        "\n",
        "\n",
        "# Convert file extensions to lowercase\n",
        "for root, dirs, files in os.walk(output_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".JPG\"):\n",
        "            # Rename the file with lowercase extension\n",
        "            os.rename(os.path.join(root, file), os.path.join(root, file.lower()))\n",
        "\n",
        "# Create a DataFrame to store predictions\n",
        "predictions_df = pd.DataFrame(columns=[\"ID\", \"extent\"])\n",
        "\n",
        "\n",
        "# Image data preprocessing for the test images\n",
        "image_size = (224, 224)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "# test_generator = test_datagen.flow_from_directory(output_folder,\n",
        "#                                                   target_size=image_size,\n",
        "#                                                   batch_size=1,\n",
        "#                                                   class_mode=None,\n",
        "#                                                   shuffle=False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(dataframe=test_data,\n",
        "                                                  directory=output_folder,\n",
        "                                                  x_col=\"filename\",\n",
        "                                                  target_size=image_size,\n",
        "                                                  batch_size=1,\n",
        "                                                  class_mode=None,\n",
        "                                                  shuffle=False)\n",
        "\n",
        "# Check the number of images found in the test directory\n",
        "num_test_images = len(test_generator.filenames)\n",
        "print(f\"Found {num_test_images} images in the test directory.\")\n",
        "\n",
        "invalid_filenames = test_data[~test_data['filename'].isin(os.listdir(output_folder))]['filename']\n",
        "print(\"Invalid Filenames:\")\n",
        "print(invalid_filenames)\n",
        "\n",
        "# Only proceed if there are images in the test directory\n",
        "if num_test_images > 0:\n",
        "    # Make predictions on the test data\n",
        "    predictions = model.predict(test_generator)\n",
        "\n",
        "    # Add \"ID\" and \"extent\" columns to the DataFrame\n",
        "    predictions_df[\"ID\"] =  test_data['ID']\n",
        "    # predictions_df[\"extent\"] = predictions.flatten()\n",
        "\n",
        "    predictions_df[\"extent\"] = 10 * ((predictions.flatten() * 100) // 10)\n",
        "    # predictions_df[\"extent\"] = (predictions.flatten() * 100).round(-1)\n",
        "\n",
        "    # Save predictions to a CSV file\n",
        "    predictions_df.to_csv(\"/content/drive/MyDrive/predictions.csv\", index=False)\n",
        "    print(\"Predictions saved to predictions.csv.\")\n",
        "\n",
        "    # Print the predictions\n",
        "    print(predictions)\n",
        "else:\n",
        "    print(\"No images found in the test directory.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}